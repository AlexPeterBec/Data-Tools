{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-and-check\" data-toc-modified-id=\"Load-and-check-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load and check</a></span></li><li><span><a href=\"#Correlation-Matrix\" data-toc-modified-id=\"Correlation-Matrix-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Correlation Matrix</a></span></li><li><span><a href=\"#Plotting-data-distribution\" data-toc-modified-id=\"Plotting-data-distribution-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Plotting data distribution</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-feature-importance\" data-toc-modified-id=\"Plot-feature-importance-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Plot feature importance</a></span></li><li><span><a href=\"#Creating-new-columns\" data-toc-modified-id=\"Creating-new-columns-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Creating new columns</a></span></li></ul></li><li><span><a href=\"#Algorithms\" data-toc-modified-id=\"Algorithms-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#RandSearchCV-XGBoostClassifier\" data-toc-modified-id=\"RandSearchCV-XGBoostClassifier-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>RandSearchCV XGBoostClassifier</a></span></li><li><span><a href=\"#RandSearchCV-Light-Gradient-Boosting-Machine\" data-toc-modified-id=\"RandSearchCV-Light-Gradient-Boosting-Machine-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>RandSearchCV Light Gradient Boosting Machine</a></span></li><li><span><a href=\"#Save-a-trained-model\" data-toc-modified-id=\"Save-a-trained-model-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Save a trained model</a></span></li></ul></li><li><span><a href=\"#Challenges\" data-toc-modified-id=\"Challenges-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenges</a></span><ul class=\"toc-item\"><li><span><a href=\"#Making-a-submission\" data-toc-modified-id=\"Making-a-submission-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Making a submission</a></span></li></ul></li><li><span><a href=\"#Metrics-and-Data-manipulation\" data-toc-modified-id=\"Metrics-and-Data-manipulation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Metrics and Data manipulation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Submissions-table\" data-toc-modified-id=\"Submissions-table-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Submissions table</a></span></li><li><span><a href=\"#Dataset-reduction-/-optimization\" data-toc-modified-id=\"Dataset-reduction-/-optimization-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Dataset reduction / optimization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T08:39:12.550272Z",
     "start_time": "2019-01-09T08:39:12.026104Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "50 visualisations with Matplotlib : \n",
    "https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "xtest = pd.read_csv('file.csv')\n",
    "\n",
    "# Counting NA values in dataframe\n",
    "print(xtest.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlations = train.corr()\n",
    "# plot correlation matrix\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(4, figsize=(15, 9))\n",
    "\n",
    "for col, n in zip(xtrain.columns[0:14], range(1, 15)):\n",
    "    \n",
    "    plt.subplot(4, 4, n)\n",
    "    plt.xlabel(col)\n",
    "    \n",
    "    plt.hist(xtrain[col], label=('x', 'y'))\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "name = \"Random Forest\"\n",
    "\n",
    "indices = np.argsort(rf.feature_importances_)[::-1][:40]\n",
    "g = sns.barplot(y=X_train.columns[indices][:40],x = rf.feature_importances_[indices][:40] , orient='h')\n",
    "g.set_xlabel(\"Relative importance\",fontsize=12)\n",
    "g.set_ylabel(\"Features\",fontsize=12)\n",
    "\n",
    "g.tick_params(labelsize=9)\n",
    "g.set_title(\"Feature importance\")\n",
    "plt.savefig('images/importance.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_fe = xtrain.copy()\n",
    "\n",
    "for column in xtrain_fe.columns:\n",
    "    new_col = column+'_sq'\n",
    "    xtrain_fe[new_col] = pow(xtrain_fe[column], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandSearchCV XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Classifier\n",
    "xgb = XGBClassifier(\n",
    "    n_jobs=-1,\n",
    "    silent=False)\n",
    "\n",
    "# Create hyperparameter options\n",
    "xgb_max_depth=[3, 5, 7, 10]             # Usual values between 3-10\n",
    "xgb_learning_rate=[0.1, 0.5, 1, 1.2]    # Makes the model more robust by shrinking the weights on each step\n",
    "xgb_n_estimators=[100, 200, 500, 1000, 1200]\n",
    "xgb_booster=['gbtree']                  #, 'gblinear', 'dart']\n",
    "xgb_reg_lambda=[1, 2]                   # L2 used to reduce overfitting\n",
    "\n",
    "hyperparameters = dict(\n",
    "    max_depth = xgb_max_depth, \n",
    "    learning_rate = xgb_learning_rate,\n",
    "    n_estimators = xgb_n_estimators,\n",
    "    booster=xgb_booster,\n",
    "    reg_lambda=xgb_lambda)\n",
    "\n",
    "# Create randomized grid search\n",
    "rscv = RandomizedSearchCV(xgb, hyperparameters, random_state=1, n_iter=50, cv=5, verbose=10, n_jobs=-1)\n",
    "# Fit randomized search\n",
    "best_model = rscv.fit(xtrain_part, ytrain_part)\n",
    "\n",
    "# View Hyperparameter Values Of Best Model\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best learning_rate:', best_model.best_estimator_.get_params()['learning_rate'])\n",
    "print('Best n_estimators:', best_model.best_estimator_.get_params()['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandSearchCV Light Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "xtrain_lgbm = xtrain.head(n=100000)\n",
    "ytrain_lgbm = ytrain.head(n=100000)\n",
    "\n",
    "# Classifier\n",
    "lgb_estimator = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt',  \n",
    "    objective='binary', \n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    metric='binary_logloss',\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(\n",
    "    num_leaves = [31, 60, 128, 160, 250], \n",
    "    reg_alpha = [0.1, 0.5],\n",
    "    min_data_in_leaf = [30, 50, 100, 300, 400],\n",
    "    n_estimators = [100, 200, 500, 1000, 2000, 5000]\n",
    "    lambda_l1 = [0, 1, 1.5],\n",
    "    lambda_l2 = [0, 1],\n",
    "    max_depth = [7],\n",
    "    )\n",
    "\n",
    "# Create randomized grid search\n",
    "rscv = RandomizedSearchCV(\n",
    "    lgb_estimator, hyperparameters,\n",
    "    n_iter=100, cv=5, \n",
    "    verbose=20, n_jobs=-1)\n",
    "\n",
    "# Fit randomized search\n",
    "best_model = rscv.fit(xtrain_lgbm, np.ravel(ytrain_lgbm))\n",
    "\n",
    "# View Hyperparameter Values Of Best Model\n",
    "print('Best num_leaves:', best_model.best_estimator_.get_params()['num_leaves'])\n",
    "print('Best reg_alpha:', best_model.best_estimator_.get_params()['reg_alpha'])\n",
    "print('Best min_data_in_leaf:', best_model.best_estimator_.get_params()['min_data_in_leaf'])\n",
    "print('Best lambda_l1:', best_model.best_estimator_.get_params()['lambda_l1'])\n",
    "print('Best lambda_l2:', best_model.best_estimator_.get_params()['lambda_l2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a trained model\n",
    "Credit : https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Save the model to disk\n",
    "filename = 'saved_lr.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " \n",
    "# Load the model from disk\n",
    "filename = 'saved_lr.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T09:02:32.364905Z",
     "start_time": "2019-01-05T09:02:32.359120Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(test_data, algorithm, filename='submission.csv'):\n",
    "    \"\"\"Creates a CSV file for challenge submission\n",
    "  \n",
    "    test_data: Description of arg1 \n",
    "    algorithm: Algo used for making prediction\n",
    "    filename: 'submission.csv'\n",
    "    \"\"\"\n",
    "    ytest = algorithm.predict(test_data)\n",
    "    np.savetxt(fichier, ytest, fmt = '%1.0d', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Metrics and Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T08:39:15.664685Z",
     "start_time": "2019-01-09T08:39:15.628507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.984650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.985016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.987663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators  learning_rate  num_leaves     score\n",
       "1           500           0.10          -1  0.984650\n",
       "2          3000           0.08          -1  0.985016\n",
       "3          5000           0.01          -1  0.987663"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_submissions = {}\n",
    "lgbm_submissions['1'] = [500, 0.1, -1, 0.98465]\n",
    "lgbm_submissions['2'] = [3000, 0.08, -1, 0.985015830111]\n",
    "lgbm_submissions['3'] = [5000, 0.01, -1, 0.987662524559]\n",
    "\n",
    "lgbm_subs_df = pd.DataFrame.from_dict(\n",
    "    lgbm_submissions, \n",
    "    orient='index',\n",
    "    columns=['n_estimators', 'learning_rate', 'num_leaves', 'score'])\n",
    "\n",
    "lgbm_subs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset reduction / optimization\n",
    "\n",
    "Fit a KNNClassifier on the training set, and find neighbors in the test set. Only keep these neighbors. This technique can filter out training samples to make it more similar to the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T16:16:17.340222Z",
     "start_time": "2019-01-09T16:16:17.121283Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def optimize_dataset(Xtrain, Ytrain, Xtest, n_neighbors=3, save_csv=False):\n",
    "    \n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=3,\n",
    "        n_jobs=-1)\n",
    "\n",
    "    print('Fitting KNNClassifier')\n",
    "    knn.fit(Xtrain, np.ravel(Ytrain))\n",
    "\n",
    "    # Find indices of similar points.\n",
    "    neighbors_idx_list = np.unique(np.ravel(knn.kneighbors(xtest)[1]))\n",
    "    \n",
    "    # Filter out with indices\n",
    "    xtrain_opt = Xtrain.iloc[neighbors_idx_list, :]\n",
    "    ytrain_opt = Ytrain.iloc[neighbors_idx_list, :]\n",
    "\n",
    "    print(\"Now we have \", xtrain_opt.shape[0], \" lines for our training set.\")\n",
    "    \n",
    "    if save_csv:\n",
    "        print(\"Writing CSV files : x_optim.csv / y_optim.csv\")\n",
    "        xtrain_opt.to_csv('x_optim.csv')\n",
    "        ytrain_opt.to_csv('y_optim.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
