{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-and-check\" data-toc-modified-id=\"Load-and-check-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load and check</a></span></li><li><span><a href=\"#Correlation-Matrix\" data-toc-modified-id=\"Correlation-Matrix-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Correlation Matrix</a></span></li><li><span><a href=\"#Plotting-data-distribution\" data-toc-modified-id=\"Plotting-data-distribution-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Plotting data distribution</a></span><ul class=\"toc-item\"><li><span><a href=\"#Time-series\" data-toc-modified-id=\"Time-series-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Time series</a></span></li></ul></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-feature-importance\" data-toc-modified-id=\"Plot-feature-importance-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Plot feature importance</a></span></li><li><span><a href=\"#Creating-new-columns\" data-toc-modified-id=\"Creating-new-columns-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Creating new columns</a></span></li><li><span><a href=\"#Merge-Rare-values\" data-toc-modified-id=\"Merge-Rare-values-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Merge Rare values</a></span></li><li><span><a href=\"#Hashing-Trick\" data-toc-modified-id=\"Hashing-Trick-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Hashing Trick</a></span></li><li><span><a href=\"#Recursive-hashing\" data-toc-modified-id=\"Recursive-hashing-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Recursive hashing</a></span></li></ul></li><li><span><a href=\"#Algorithms\" data-toc-modified-id=\"Algorithms-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#RandSearchCV-XGBoostClassifier\" data-toc-modified-id=\"RandSearchCV-XGBoostClassifier-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>RandSearchCV XGBoostClassifier</a></span></li><li><span><a href=\"#RandSearchCV-Light-Gradient-Boosting-Machine\" data-toc-modified-id=\"RandSearchCV-Light-Gradient-Boosting-Machine-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>RandSearchCV Light Gradient Boosting Machine</a></span></li><li><span><a href=\"#Score-evolution-with-1-parameter-variation\" data-toc-modified-id=\"Score-evolution-with-1-parameter-variation-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Score evolution with 1-parameter variation</a></span></li><li><span><a href=\"#Save-a-trained-model\" data-toc-modified-id=\"Save-a-trained-model-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Save a trained model</a></span></li><li><span><a href=\"#Plot-learning-curve\" data-toc-modified-id=\"Plot-learning-curve-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Plot learning curve</a></span></li></ul></li><li><span><a href=\"#Challenges\" data-toc-modified-id=\"Challenges-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenges</a></span><ul class=\"toc-item\"><li><span><a href=\"#Making-a-submission\" data-toc-modified-id=\"Making-a-submission-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Making a submission</a></span></li></ul></li><li><span><a href=\"#Metrics-and-Data-manipulation\" data-toc-modified-id=\"Metrics-and-Data-manipulation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Metrics and Data manipulation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-ROC-curve\" data-toc-modified-id=\"Plot-ROC-curve-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Plot ROC curve</a></span></li><li><span><a href=\"#Evaluate-the-model\" data-toc-modified-id=\"Evaluate-the-model-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Evaluate the model</a></span></li><li><span><a href=\"#Submissions-table\" data-toc-modified-id=\"Submissions-table-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Submissions table</a></span></li><li><span><a href=\"#Dataset-reduction-/-optimization\" data-toc-modified-id=\"Dataset-reduction-/-optimization-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Dataset reduction / optimization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T08:07:23.407211Z",
     "start_time": "2019-01-18T08:07:22.875668Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "50 visualisations with Matplotlib : \n",
    "https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "xtest = pd.read_csv('file.csv')\n",
    "\n",
    "# Counting NA values in dataframe\n",
    "print(xtest.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlations = train.corr()\n",
    "# plot correlation matrix\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matplotlib\n",
    "http://www.labri.fr/perso/nrougier/teaching/matplotlib/matplotlib.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quantitative variables distribution\n",
    "plt.figure(4, figsize=(15, 9))\n",
    "\n",
    "for col, n in zip(xtrain.columns[0:14], range(1, 15)):\n",
    "    \n",
    "    plt.subplot(4, 4, n)\n",
    "    plt.xlabel(col)\n",
    "    \n",
    "    plt.hist(xtrain[col], label=('x', 'y'))\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classes histogram\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.hist(digits['target'], bins=19 , color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ias_brut = grippal[['PERIODE','IAS_brut']]\n",
    "\n",
    "ias_brut.plot(figsize=(15, 4))\n",
    "plt.xticks(ticks=range(0, ias_brut.shape[0], 500), labels=ias_brut['PERIODE'][::500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "name = \"Random Forest\"\n",
    "\n",
    "indices = np.argsort(rf.feature_importances_)[::-1][:40]\n",
    "g = sns.barplot(y=X_train.columns[indices][:40],x = rf.feature_importances_[indices][:40] , orient='h')\n",
    "g.set_xlabel(\"Relative importance\",fontsize=12)\n",
    "g.set_ylabel(\"Features\",fontsize=12)\n",
    "\n",
    "g.tick_params(labelsize=9)\n",
    "g.set_title(\"Feature importance\")\n",
    "plt.savefig('images/importance.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_fe = xtrain.copy()\n",
    "\n",
    "for column in xtrain_fe.columns:\n",
    "    new_col = column+'_sq'\n",
    "    xtrain_fe[new_col] = pow(xtrain_fe[column], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Rare values\n",
    "\n",
    "With this function, we assign the value 'isRare' to values appearing less than a X times. X being the threshold of our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MergeRareTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names, threshold):\n",
    "        self.col_names = col_names\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        X = pd.DataFrame(X)\n",
    "        counts_dict_list = []\n",
    "        \n",
    "        for i in range(len(self.col_names)):\n",
    "            \n",
    "            serie = X[self.col_names[i]].value_counts()  \n",
    "            rare_indexes = serie[serie<self.threshold[i]].index \n",
    "            frequent_indexes = serie[serie>=self.threshold[i]].index  \n",
    "            dico = {x:'isRare' for x in rare_indexes}\n",
    "            dico.update({x: str(x) for x in frequent_indexes})\n",
    "            counts_dict_list.append(dico) \n",
    "            \n",
    "        self.counts_dict_list_ = counts_dict_list\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        Xt = pd.DataFrame()\n",
    "        for col, count_dict in zip(self.col_names, self.counts_dict_list_):\n",
    "            Xt[col] = X[col].apply(lambda x:count_dict[x] if x in count_dict else 'isRare')\n",
    "\n",
    "        return Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying function\n",
    "mg = MergeRareTransformer(col_names=Xtrain.columns, threshold=[20]*len(Xtrain.columns))\n",
    "\n",
    "Xtrain_mg = mg.fit_transform(Xtrain)\n",
    "Xtest_mg = mg.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "# Create new dataframes\n",
    "Xtrain_ha = pd.DataFrame()\n",
    "Xtest_ha = pd.DataFrame()\n",
    "\n",
    "# Apply hashing \n",
    "for col in Xtrain_mg.columns:\n",
    "    Xtrain_ha[col] = Xtrain_mg[col].apply(lambda x: hash(str(x)) % 1000000) # Fill here defining a lambda function\n",
    "    Xtest_ha[col] = Xtest_mg[col].apply(lambda x: hash(str(x)) % 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Continue with hashed dataframes\n",
    "Xtrain_haha = pd.DataFrame(Xtrain_ha).copy()\n",
    "Xtest_haha = pd.DataFrame(Xtest_ha).copy()\n",
    "\n",
    "n_hash = 3\n",
    "cols = Xtrain_ha.columns\n",
    "for l in range(n_hash):\n",
    "    for col in cols:\n",
    "        Xtrain_haha[col + '-hash'] = Xtrain_haha[col].apply(lambda x:hash(str(x)))\n",
    "        Xtest_haha[col + '-hash'] = Xtest_haha[col].apply(lambda x:hash(str(x)))\n",
    "    cols = [col + '-hash' for col in cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandSearchCV XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Classifier\n",
    "xgb = XGBClassifier(\n",
    "    n_jobs=-1,\n",
    "    silent=False)\n",
    "\n",
    "# Create hyperparameter options\n",
    "xgb_max_depth=[3, 5, 7, 10]             # Usual values between 3-10\n",
    "xgb_learning_rate=[0.1, 0.5, 1, 1.2]    # Makes the model more robust by shrinking the weights on each step\n",
    "xgb_n_estimators=[100, 200, 500, 1000, 1200]\n",
    "xgb_booster=['gbtree']                  #, 'gblinear', 'dart']\n",
    "xgb_reg_lambda=[1, 2]                   # L2 used to reduce overfitting\n",
    "\n",
    "hyperparameters = dict(\n",
    "    max_depth = xgb_max_depth, \n",
    "    learning_rate = xgb_learning_rate,\n",
    "    n_estimators = xgb_n_estimators,\n",
    "    booster=xgb_booster,\n",
    "    reg_lambda=xgb_lambda)\n",
    "\n",
    "# Create randomized grid search\n",
    "rscv = RandomizedSearchCV(xgb, hyperparameters, random_state=1, n_iter=50, cv=5, verbose=10, n_jobs=-1)\n",
    "# Fit randomized search\n",
    "best_model = rscv.fit(xtrain_part, ytrain_part)\n",
    "\n",
    "# View Hyperparameter Values Of Best Model\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best learning_rate:', best_model.best_estimator_.get_params()['learning_rate'])\n",
    "print('Best n_estimators:', best_model.best_estimator_.get_params()['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandSearchCV Light Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "xtrain_lgbm = xtrain.head(n=100000)\n",
    "ytrain_lgbm = ytrain.head(n=100000)\n",
    "\n",
    "# Classifier\n",
    "lgb_estimator = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt',  \n",
    "    objective='binary', \n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    metric='binary_logloss',\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(\n",
    "    num_leaves = [31, 60, 128, 160, 250], \n",
    "    reg_alpha = [0.1, 0.5],\n",
    "    min_data_in_leaf = [30, 50, 100, 300, 400],\n",
    "    n_estimators = [100, 200, 500, 1000, 2000, 5000]\n",
    "    lambda_l1 = [0, 1, 1.5],\n",
    "    lambda_l2 = [0, 1],\n",
    "    max_depth = [7],\n",
    "    )\n",
    "\n",
    "# Create randomized grid search\n",
    "rscv = RandomizedSearchCV(\n",
    "    lgb_estimator, hyperparameters,\n",
    "    n_iter=100, cv=5, \n",
    "    verbose=20, n_jobs=-1)\n",
    "\n",
    "# Fit randomized search\n",
    "best_model = rscv.fit(xtrain_lgbm, np.ravel(ytrain_lgbm))\n",
    "\n",
    "# View Hyperparameter Values Of Best Model\n",
    "print('Best num_leaves:', best_model.best_estimator_.get_params()['num_leaves'])\n",
    "print('Best reg_alpha:', best_model.best_estimator_.get_params()['reg_alpha'])\n",
    "print('Best min_data_in_leaf:', best_model.best_estimator_.get_params()['min_data_in_leaf'])\n",
    "print('Best lambda_l1:', best_model.best_estimator_.get_params()['lambda_l1'])\n",
    "print('Best lambda_l2:', best_model.best_estimator_.get_params()['lambda_l2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score evolution with 1-parameter variation\n",
    "For this snippet, data needs to be train-test splitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For DecisionTree\n",
    "accuracy_gini = []\n",
    "accuracy_entropy = []\n",
    "depth = [1, 2, 4, 6, 8, 10, 11, 12, 13, 14, 15, 20]\n",
    "for i in depth:\n",
    "    class_tree = tree.DecisionTreeClassifier(criterion='gini', max_depth=i)\n",
    "    class_tree.fit(x1_train, y1_train)\n",
    "    y1_pred = class_tree.predict(x1_test)\n",
    "    accuracy_gini.append(metrics.accuracy_score(y1_test, y1_pred, normalize=False))\n",
    "    \n",
    "    class_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=i)\n",
    "    class_tree.fit(x1_train, y1_train)\n",
    "    y1_pred = class_tree.predict(x1_test)\n",
    "    accuracy_entropy.append(metrics.accuracy_score(y1_test, y1_pred, normalize=False))\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(depth, accuracy_gini)\n",
    "plt.plot(depth, accuracy_entropy)\n",
    "plt.xlabel(\"Paramètre MaxDepth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"gini\", \"entropy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For XGBC\n",
    "accuracy_one = []\n",
    "accuracy_two = []\n",
    "depth = [1, 2, 4, 6, 7, 8, 10, 11, 12]\n",
    "\n",
    "for i in depth:\n",
    "    print(\"Round \"+str(i))\n",
    "    # Model 1\n",
    "    xgb1 = XGBClassifier(n_jobs=-1, max_depth=i, n_estimators=10)\n",
    "    xgb1.fit(X_train, np.ravel(y_train))\n",
    "    y1_pred = xgb1.predict(X_test)\n",
    "    accuracy_one.append(accuracy_score(y_test, y1_pred, normalize=False))\n",
    "    \n",
    "    # Model2\n",
    "    xgb2 = XGBClassifier(n_jobs=-1, max_depth=i, n_estimators=50)\n",
    "    xgb2.fit(X_train, np.ravel(y_train))\n",
    "    y1_pred = xgb2.predict(X_test)\n",
    "    accuracy_two.append(accuracy_score(y_test, y1_pred, normalize=False))\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(depth, accuracy_one)\n",
    "plt.plot(depth, accuracy_two)\n",
    "plt.xlabel(\"Paramètre MaxDepth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"100 estimators\", \"200 estimators\"])\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('images/max_depth_XGB.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a trained model\n",
    "Source : https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Save the model to disk\n",
    "filename = 'saved_lr.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " \n",
    "# Load the model from disk\n",
    "filename = 'saved_lr.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "\n",
    "def save_model(model, filename):\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "def load_model(model, filename):\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    return pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "class_learn = tree.DecisionTreeClassifier(criterion='entropy', max_depth=18)\n",
    "class_learn.fit(data_train, target_train)\n",
    "\n",
    "plot_learning_curve(class_learn, 'Classifier Learning Curve', digits.data, digits.target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T09:02:32.364905Z",
     "start_time": "2019-01-05T09:02:32.359120Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(test_data, algorithm, filename='submission.csv'):\n",
    "    \"\"\"Creates a CSV file for challenge submission\n",
    "  \n",
    "    test_data: Description of arg1 \n",
    "    algorithm: Algo used for making prediction\n",
    "    filename: 'submission.csv'\n",
    "    \"\"\"\n",
    "    ytest = algorithm.predict(test_data)\n",
    "    np.savetxt(fichier, ytest, fmt = '%1.0d', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Metrics and Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(ytest, soft):\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(ytest, soft[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, lgbm.predict_proba(X_test_fe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, xtrain, ytrain, xtest, ytest):\n",
    "    \n",
    "    print(\"Fitting..\")\n",
    "    lgbm.fit(xtrain, np.ravel(y_train))\n",
    "\n",
    "    print(\"Predicting..\")\n",
    "    soft = lgbm.predict_proba(xtest)\n",
    "    hard = lgbm.predict(xtest)\n",
    "    \n",
    "    print(\"Scoring :\")\n",
    "    print(name, \" \", accuracy_score(ytest, hard, normalize=True, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions table\n",
    "Fill a score dictionnary along the way, and finally display it as a frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T08:39:15.664685Z",
     "start_time": "2019-01-09T08:39:15.628507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.984650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.985016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.987663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators  learning_rate  num_leaves     score\n",
       "1           500           0.10          -1  0.984650\n",
       "2          3000           0.08          -1  0.985016\n",
       "3          5000           0.01          -1  0.987663"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_submissions = {}\n",
    "lgbm_submissions['1'] = [500, 0.1, -1, 0.98465]\n",
    "lgbm_submissions['2'] = [3000, 0.08, -1, 0.985015830111]\n",
    "lgbm_submissions['3'] = [5000, 0.01, -1, 0.987662524559]\n",
    "\n",
    "lgbm_subs_df = pd.DataFrame.from_dict(\n",
    "    lgbm_submissions, \n",
    "    orient='index',\n",
    "    columns=['n_estimators', 'learning_rate', 'num_leaves', 'score'])\n",
    "\n",
    "lgbm_subs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset reduction / optimization\n",
    "\n",
    "Fit a KNNClassifier on the training set, and find neighbors in the test set. Only keep these neighbors. This technique can filter out training samples to make it more similar to the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T16:16:17.340222Z",
     "start_time": "2019-01-09T16:16:17.121283Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def optimize_dataset(Xtrain, Ytrain, Xtest, n_neighbors=3, save_csv=False):\n",
    "    \n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=3,\n",
    "        n_jobs=-1)\n",
    "\n",
    "    print('Fitting KNNClassifier')\n",
    "    knn.fit(Xtrain, np.ravel(Ytrain))\n",
    "\n",
    "    # Find indices of similar points.\n",
    "    neighbors_idx_list = np.unique(np.ravel(knn.kneighbors(xtest)[1]))\n",
    "    \n",
    "    # Filter out with indices\n",
    "    xtrain_opt = Xtrain.iloc[neighbors_idx_list, :]\n",
    "    ytrain_opt = Ytrain.iloc[neighbors_idx_list, :]\n",
    "\n",
    "    print(\"Now we have \", xtrain_opt.shape[0], \" lines for our training set.\")\n",
    "    \n",
    "    if save_csv:\n",
    "        print(\"Writing CSV files : x_optim.csv / y_optim.csv\")\n",
    "        xtrain_opt.to_csv('x_optim.csv')\n",
    "        ytrain_opt.to_csv('y_optim.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
